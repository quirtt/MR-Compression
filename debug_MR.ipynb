{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import time\n",
    "from numpy import mean\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language datasets, filtering into lowercase text\n",
    "\n",
    "### Average increase in compression rate: <span style=\"color:red;\">7.509938589043936 %</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "macbeth = gutenberg.raw('austen-persuasion.txt')\n",
    "fileids = gutenberg.fileids()\n",
    "print(fileids)\n",
    "data = macbeth.replace('\\n', '')\n",
    "def MR(data):\n",
    "    def filter(str):\n",
    "        str = str.lower()\n",
    "        i = [x for x in range(97)] \n",
    "        j = [x for x in range(123,256)]\n",
    "        k = i + j\n",
    "        for c in [chr(i) for i in k]:\n",
    "            str = str.replace(c, '')\n",
    "        return str\n",
    "    \n",
    "    def lzw_encode(data):\n",
    "        # Build the dictionary.\n",
    "        # lowercase!!\n",
    "        data = filter(data)\n",
    "        global initial_size_MR \n",
    "        initial_size_MR = len(data)\n",
    "        hadict = [{} for _ in range(26)]\n",
    "\n",
    "        p = \"\"\n",
    "        result = []\n",
    "        for i in range(26):\n",
    "            hadict[i][chr(ord('a') + i)] = 0\n",
    "\n",
    "        for c in data:\n",
    "            pc = p + c\n",
    "            if pc in hadict[ord(c)- ord('a')]:\n",
    "                p = pc\n",
    "            else:\n",
    "                val = ord(c)- ord('a')\n",
    "                if(p == \"\"): result.append(\"#\")\n",
    "                elif hadict[ord(p[-1]) - ord('a')][p] == 0 : result.append('#'+p[-1])\n",
    "                else: result.append(str(hadict[ord(p[-1]) - ord('a')][p]) + p[-1])\n",
    "                # if dict_size < 4096:  # Typically the dictionary size is limited to 4096 entries.\n",
    "                hadict[val][pc] = len(hadict[val])\n",
    "                p = c\n",
    "\n",
    "        if p:\n",
    "            val = ord(c)- ord('a')\n",
    "            result.append(str(hadict[val][p]) + p[-1])\n",
    "        return \"\".join(result)\n",
    "    return lzw_encode(data)\n",
    "def lzw_encode(data):\n",
    "    # Build the dictionary.\n",
    "    def filter(str):\n",
    "        str = str.lower()\n",
    "        i = [x for x in range(97)] \n",
    "        j = [x for x in range(123,256)]\n",
    "        k = i + j\n",
    "        for c in [chr(i) for i in k]:\n",
    "            str = str.replace(c, '')\n",
    "        return str\n",
    "    data = filter(data)\n",
    "    global initial_size_lzw \n",
    "    initial_size_lzw = len(data)\n",
    "    max_dict_size = 26\n",
    "    dictionary = {chr(i): i for i in range(97,123)}\n",
    "    dict_size = max_dict_size\n",
    "    p = \"\"\n",
    "    result = []\n",
    "\n",
    "    for c in data:\n",
    "        pc = p + c\n",
    "        if pc in dictionary:\n",
    "            p = pc\n",
    "        else:\n",
    "            if len(p) == 1: result.append('#'+p)\n",
    "            else: result.append(str(dictionary[p]))\n",
    "            # if dict_size < 4096:  # Typically the dictionary size is limited to 4096 entries.\n",
    "            dictionary[pc] = str(dict_size)\n",
    "            dict_size += 1\n",
    "            p = c\n",
    "\n",
    "    if p:\n",
    "        result.append(str(dictionary[p]))\n",
    "\n",
    "    return \"\".join(result)\n",
    "def lzw_decode(compressed):\n",
    "    # Build the dictionary.\n",
    "    max_dict_size = 256\n",
    "    dictionary = {i: chr(i) for i in range(max_dict_size)}\n",
    "    dict_size = max_dict_size\n",
    "\n",
    "    # Decoding first value\n",
    "    result = []\n",
    "    w = chr(compressed.pop(0))\n",
    "    result.append(w)\n",
    "\n",
    "    for k in compressed:\n",
    "        if k in dictionary:\n",
    "            entry = dictionary[k]\n",
    "        elif k == dict_size:\n",
    "            entry = w + w[0]\n",
    "        else:\n",
    "            raise ValueError(\"Bad compressed k: %s\" % k)\n",
    "        \n",
    "        result.append(entry)\n",
    "\n",
    "        # Add w+entry[0] to the dictionary.\n",
    "\n",
    "        dictionary[dict_size] = w + entry[0]\n",
    "        dict_size += 1\n",
    "\n",
    "        w = entry\n",
    "\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileId: austen-emma.txt\n",
      "% increase in compression rate:  4.6926396298845425\n",
      "% increase in time taken:  42.473088580277604\n",
      "absolute increase in time taken:  0.08249115943908691 seconds\n",
      "\n",
      "FileId: austen-persuasion.txt\n",
      "% increase in compression rate:  5.41221221330951\n",
      "% increase in time taken:  41.52343520163805\n",
      "absolute increase in time taken:  0.04206442832946777 seconds\n",
      "\n",
      "FileId: austen-sense.txt\n",
      "% increase in compression rate:  4.531457984609498\n",
      "% increase in time taken:  38.11244329228775\n",
      "absolute increase in time taken:  0.05608320236206055 seconds\n",
      "\n",
      "FileId: bible-kjv.txt\n",
      "% increase in compression rate:  4.199595625510391\n",
      "% increase in time taken:  24.28085780001476\n",
      "absolute increase in time taken:  0.2674713134765625 seconds\n",
      "\n",
      "FileId: blake-poems.txt\n",
      "% increase in compression rate:  7.221242758968085\n",
      "% increase in time taken:  37.189731609546314\n",
      "absolute increase in time taken:  0.003340005874633789 seconds\n",
      "\n",
      "FileId: bryant-stories.txt\n",
      "% increase in compression rate:  6.692308944236185\n",
      "% increase in time taken:  42.722105577566744\n",
      "absolute increase in time taken:  0.023189067840576172 seconds\n",
      "\n",
      "FileId: burgess-busterbrown.txt\n",
      "% increase in compression rate:  6.457223440927609\n",
      "% increase in time taken:  45.33147022765921\n",
      "absolute increase in time taken:  0.008198738098144531 seconds\n",
      "\n",
      "FileId: carroll-alice.txt\n",
      "% increase in compression rate:  6.861679899707481\n",
      "% increase in time taken:  41.73192608889641\n",
      "absolute increase in time taken:  0.012933969497680664 seconds\n",
      "\n",
      "FileId: chesterton-ball.txt\n",
      "% increase in compression rate:  5.721764617847794\n",
      "% increase in time taken:  30.516069992512683\n",
      "absolute increase in time taken:  0.039354801177978516 seconds\n",
      "\n",
      "FileId: chesterton-brown.txt\n",
      "% increase in compression rate:  6.092538798568425\n",
      "% increase in time taken:  40.94580153901505\n",
      "absolute increase in time taken:  0.03635907173156738 seconds\n",
      "\n",
      "FileId: chesterton-thursday.txt\n",
      "% increase in compression rate:  6.281186147429438\n",
      "% increase in time taken:  41.24742808730383\n",
      "absolute increase in time taken:  0.0292510986328125 seconds\n",
      "\n",
      "FileId: edgeworth-parents.txt\n",
      "% increase in compression rate:  5.203508994538104\n",
      "% increase in time taken:  33.854834249230315\n",
      "absolute increase in time taken:  0.07034087181091309 seconds\n",
      "\n",
      "FileId: melville-moby_dick.txt\n",
      "% increase in compression rate:  5.515399103544121\n",
      "% increase in time taken:  44.3329120721868\n",
      "absolute increase in time taken:  0.13147592544555664 seconds\n",
      "\n",
      "FileId: milton-paradise.txt\n",
      "% increase in compression rate:  5.651157554750714\n",
      "% increase in time taken:  40.7982266734047\n",
      "absolute increase in time taken:  0.04234576225280762 seconds\n",
      "\n",
      "FileId: shakespeare-caesar.txt\n",
      "% increase in compression rate:  7.2471883339687375\n",
      "% increase in time taken:  31.617048585023795\n",
      "absolute increase in time taken:  0.009631872177124023 seconds\n",
      "\n",
      "FileId: shakespeare-hamlet.txt\n",
      "% increase in compression rate:  7.21399136577708\n",
      "% increase in time taken:  43.06752363766617\n",
      "absolute increase in time taken:  0.015192985534667969 seconds\n",
      "\n",
      "FileId: shakespeare-macbeth.txt\n",
      "% increase in compression rate:  7.082066465738146\n",
      "% increase in time taken:  43.541107986133824\n",
      "absolute increase in time taken:  0.00952291488647461 seconds\n",
      "\n",
      "FileId: whitman-leaves.txt\n",
      "% increase in compression rate:  5.128058551589461\n",
      "% increase in time taken:  41.454892283469334\n",
      "absolute increase in time taken:  0.06528425216674805 seconds\n",
      "\n",
      "\u001b[91mAverage increase in compression rate 7.509938589043936 %\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "# decompressed = lzw_decode(compressed)\n",
    "# print(\"Decompressed:\", decompressed)\n",
    "for fileid in fileids:\n",
    "    # if fileid == macbeth:\n",
    "    data = gutenberg.raw(fileid)\n",
    "    print('FileId:', fileid)\n",
    "    start_time_MR = time.time()\n",
    "    encoded_MR = MR(data)\n",
    "    end_time_MR = time.time()\n",
    "    total_time_MR = end_time_MR - start_time_MR\n",
    "    final_size_MR = len(encoded_MR)\n",
    "    start_time_lzw = time.time()\n",
    "    encoded_lzw = lzw_encode(data)\n",
    "    end_time_lzw = time.time()\n",
    "    total_time_lzw = end_time_lzw - start_time_lzw\n",
    "    final_size_lzw = len(encoded_lzw)\n",
    "    compMR = (initial_size_MR-final_size_MR)/initial_size_MR * 100\n",
    "    compLZW = (initial_size_lzw-final_size_lzw)/initial_size_lzw * 100\n",
    "    CompInc = compMR - compLZW\n",
    "    CompRateInc.append(CompInc)\n",
    "    # print('Compression Rate MR: ', compMR)\n",
    "    # print('Time Taken MR:', total_time_MR)\n",
    "    # print('Compression Rate LZW: ', compLZW)\n",
    "    # print('Time Taken LZW:', total_time_lzw)\n",
    "    print('% increase in compression rate: ', CompInc)\n",
    "    print('% increase in time taken: ', (total_time_MR-total_time_lzw)/total_time_lzw * 100)\n",
    "    print('absolute increase in time taken: ', total_time_MR - total_time_lzw, 'seconds')\n",
    "    print()\n",
    "print('\\033[91mAverage increase in compression rate', mean(CompRateInc), '%\\033[0m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language datasets, no filtering (complete text using ASCII)\n",
    "\n",
    "### Average increase in compression rate: <span style=\"color:red;\">8.286985093818535 %</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "macbeth = gutenberg.raw('austen-persuasion.txt')\n",
    "fileids = gutenberg.fileids()\n",
    "print(fileids)\n",
    "data = macbeth.replace('\\n', '')\n",
    "\n",
    "def MR(data):\n",
    "    def lzw_encode(data):\n",
    "        # Build the dictionary.\n",
    "        global initial_size_MR \n",
    "        initial_size_MR = len(data)\n",
    "        hadict = [{} for _ in range(256)]\n",
    "\n",
    "        p = \"\"\n",
    "        result = []\n",
    "        for i in range(256):\n",
    "            hadict[i][chr(i)] = 0\n",
    "\n",
    "        for c in data:\n",
    "            pc = p + c\n",
    "            if pc in hadict[ord(c)]:\n",
    "                p = pc\n",
    "            else:\n",
    "                val = ord(c)\n",
    "                if(p == \"\"): result.append(\"#\")\n",
    "                elif hadict[ord(p[-1])][p] == 0 : result.append('#'+p[-1])\n",
    "                else: result.append(str(hadict[ord(p[-1])][p]) + p[-1])\n",
    "                # if dict_size < 4096:  # Typically the dictionary size is limited to 4096 entries.\n",
    "                hadict[val][pc] = len(hadict[val])\n",
    "                p = c\n",
    "\n",
    "        if p:\n",
    "            val = ord(c)\n",
    "            result.append(str(hadict[val][p]) + p[-1])\n",
    "        return \"\".join(result)\n",
    "    return lzw_encode(data)\n",
    "def lzw_encode(data):\n",
    "    # Build the dictionary.\n",
    "    global initial_size_lzw \n",
    "    initial_size_lzw = len(data)\n",
    "    max_dict_size = 256\n",
    "    dictionary = {chr(i): i for i in range(256)}\n",
    "    dict_size = max_dict_size\n",
    "    p = \"\"\n",
    "    result = []\n",
    "\n",
    "    for c in data:\n",
    "        pc = p + c\n",
    "        if pc in dictionary:\n",
    "            p = pc\n",
    "        else:\n",
    "            if len(p) == 1: result.append('#'+p)\n",
    "            else: result.append(str(dictionary[p]))\n",
    "            # if dict_size < 4096:  # Typically the dictionary size is limited to 4096 entries.\n",
    "            dictionary[pc] = str(dict_size)\n",
    "            dict_size += 1\n",
    "            p = c\n",
    "\n",
    "    if p:\n",
    "        result.append(str(dictionary[p]))\n",
    "\n",
    "    return \"\".join(result)\n",
    "def lzw_decode(compressed):\n",
    "    # Build the dictionary.\n",
    "    max_dict_size = 256\n",
    "    dictionary = {i: chr(i) for i in range(max_dict_size)}\n",
    "    dict_size = max_dict_size\n",
    "\n",
    "    # Decoding first value\n",
    "    result = []\n",
    "    w = chr(compressed.pop(0))\n",
    "    result.append(w)\n",
    "\n",
    "    for k in compressed:\n",
    "        if k in dictionary:\n",
    "            entry = dictionary[k]\n",
    "        elif k == dict_size:\n",
    "            entry = w + w[0]\n",
    "        else:\n",
    "            raise ValueError(\"Bad compressed k: %s\" % k)\n",
    "        \n",
    "        result.append(entry)\n",
    "\n",
    "        # Add w+entry[0] to the dictionary.\n",
    "\n",
    "        dictionary[dict_size] = w + entry[0]\n",
    "        dict_size += 1\n",
    "\n",
    "        w = entry\n",
    "\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileId: austen-emma.txt\n",
      "% increase in compression rate:  6.134458233895595\n",
      "% increase in time taken:  29.788135306496688\n",
      "absolute increase in time taken:  0.05247807502746582 seconds\n",
      "\n",
      "FileId: austen-persuasion.txt\n",
      "% increase in compression rate:  6.514372968011461\n",
      "% increase in time taken:  28.389766309487115\n",
      "absolute increase in time taken:  0.025911331176757812 seconds\n",
      "\n",
      "FileId: austen-sense.txt\n",
      "% increase in compression rate:  5.984054013093182\n",
      "% increase in time taken:  28.069633738524985\n",
      "absolute increase in time taken:  0.037062883377075195 seconds\n",
      "\n",
      "FileId: bible-kjv.txt\n",
      "% increase in compression rate:  5.336875201093861\n",
      "% increase in time taken:  19.201746789150455\n",
      "absolute increase in time taken:  0.19786286354064941 seconds\n",
      "\n",
      "FileId: blake-poems.txt\n",
      "% increase in compression rate:  11.328073808088487\n",
      "% increase in time taken:  36.27633484029194\n",
      "absolute increase in time taken:  0.002927064895629883 seconds\n",
      "\n",
      "FileId: bryant-stories.txt\n",
      "% increase in compression rate:  8.885138250233524\n",
      "% increase in time taken:  27.002207045495446\n",
      "absolute increase in time taken:  0.013592958450317383 seconds\n",
      "\n",
      "FileId: burgess-busterbrown.txt\n",
      "% increase in compression rate:  10.253593659567935\n",
      "% increase in time taken:  31.90861574776597\n",
      "absolute increase in time taken:  0.005414485931396484 seconds\n",
      "\n",
      "FileId: carroll-alice.txt\n",
      "% increase in compression rate:  10.325842307559125\n",
      "% increase in time taken:  31.146338481600715\n",
      "absolute increase in time taken:  0.009111166000366211 seconds\n",
      "\n",
      "FileId: chesterton-ball.txt\n",
      "% increase in compression rate:  7.0482019892884455\n",
      "% increase in time taken:  29.197112364981354\n",
      "absolute increase in time taken:  0.0269320011138916 seconds\n",
      "\n",
      "FileId: chesterton-brown.txt\n",
      "% increase in compression rate:  7.3420734871344635\n",
      "% increase in time taken:  29.893036532732125\n",
      "absolute increase in time taken:  0.023993730545043945 seconds\n",
      "\n",
      "FileId: chesterton-thursday.txt\n",
      "% increase in compression rate:  8.038062553622963\n",
      "% increase in time taken:  31.27117297838463\n",
      "absolute increase in time taken:  0.02018141746520996 seconds\n",
      "\n",
      "FileId: edgeworth-parents.txt\n",
      "% increase in compression rate:  7.125961602210536\n",
      "% increase in time taken:  26.827588301393995\n",
      "absolute increase in time taken:  0.05265188217163086 seconds\n",
      "\n",
      "FileId: melville-moby_dick.txt\n",
      "% increase in compression rate:  8.012614743481443\n",
      "% increase in time taken:  29.878289001893567\n",
      "absolute increase in time taken:  0.07847476005554199 seconds\n",
      "\n",
      "FileId: milton-paradise.txt\n",
      "% increase in compression rate:  7.003972491563794\n",
      "% increase in time taken:  31.26202611970851\n",
      "absolute increase in time taken:  0.0291292667388916 seconds\n",
      "\n",
      "FileId: shakespeare-caesar.txt\n",
      "% increase in compression rate:  10.748820229721307\n",
      "% increase in time taken:  32.9231698931005\n",
      "absolute increase in time taken:  0.0075337886810302734 seconds\n",
      "\n",
      "FileId: shakespeare-hamlet.txt\n",
      "% increase in compression rate:  10.487411054696373\n",
      "% increase in time taken:  33.22727035626728\n",
      "absolute increase in time taken:  0.010631084442138672 seconds\n",
      "\n",
      "FileId: shakespeare-macbeth.txt\n",
      "% increase in compression rate:  11.612241033970761\n",
      "% increase in time taken:  29.838644474988165\n",
      "absolute increase in time taken:  0.006159305572509766 seconds\n",
      "\n",
      "FileId: whitman-leaves.txt\n",
      "% increase in compression rate:  6.98396406150039\n",
      "% increase in time taken:  31.5174745126695\n",
      "absolute increase in time taken:  0.044835805892944336 seconds\n",
      "\n",
      "\u001b[91mAverage increase in compression rate 8.286985093818535 %\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# decompressed = lzw_decode(compressed)\n",
    "# print(\"Decompressed:\", decompressed)\n",
    "CompRateInc = []\n",
    "for fileid in fileids:\n",
    "    # if fileid == macbeth:\n",
    "    data = gutenberg.raw(fileid)\n",
    "    print('FileId:', fileid)\n",
    "    start_time_MR = time.time()\n",
    "    encoded_MR = MR(data)\n",
    "    end_time_MR = time.time()\n",
    "    total_time_MR = end_time_MR - start_time_MR\n",
    "    final_size_MR = len(encoded_MR)\n",
    "    start_time_lzw = time.time()\n",
    "    encoded_lzw = lzw_encode(data)\n",
    "    end_time_lzw = time.time()\n",
    "    total_time_lzw = end_time_lzw - start_time_lzw\n",
    "    final_size_lzw = len(encoded_lzw)\n",
    "    compMR = (initial_size_MR-final_size_MR)/initial_size_MR * 100\n",
    "    compLZW = (initial_size_lzw-final_size_lzw)/initial_size_lzw * 100\n",
    "    CompInc = compMR - compLZW\n",
    "    CompRateInc.append(CompInc)\n",
    "    # print('Compression Rate MR: ', compMR)\n",
    "    # print('Time Taken MR:', total_time_MR)\n",
    "    # print('Compression Rate LZW: ', compLZW)\n",
    "    # print('Time Taken LZW:', total_time_lzw)\n",
    "    print('% increase in compression rate: ', CompInc)\n",
    "    print('% increase in time taken: ', (total_time_MR-total_time_lzw)/total_time_lzw * 100)\n",
    "    print('absolute increase in time taken: ', total_time_MR - total_time_lzw, 'seconds')\n",
    "    print()\n",
    "print('\\033[91mAverage increase in compression rate', mean(CompRateInc), '%\\033[0m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language datasets, no filtering (complete ASCII), 8bit fixed size encodings\n",
    "\n",
    "### Average increase in compression rate: <span style=\"color:red;\">?? %</span>\n",
    "### INCOMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "macbeth = gutenberg.raw('austen-persuasion.txt')\n",
    "fileids = gutenberg.fileids()\n",
    "print(fileids)\n",
    "data = macbeth.replace('\\n', '')\n",
    "\n",
    "def MR(data):\n",
    "    def lzw_encode(data):\n",
    "        # Build the dictionary.\n",
    "        # lowercase!!\n",
    "        global initial_size_MR \n",
    "        initial_size_MR = sys.getsizeof(data)\n",
    "        hadict = [{} for _ in range(256)]\n",
    "\n",
    "        p = \"\"\n",
    "        result = []\n",
    "        for i in range(256):\n",
    "            hadict[i][chr(i)] = 0\n",
    "\n",
    "        for c in data:\n",
    "            pc = p + c\n",
    "            if pc in hadict[ord(c)]:\n",
    "                p = pc\n",
    "            else:\n",
    "                val = ord(c)\n",
    "                if(p == \"\"): result.append(\"#\")\n",
    "                elif hadict[ord(p[-1])][p] == 0 : result.append('#'+p[-1])\n",
    "                else: result.append(str(hadict[ord(p[-1])][p]) + p[-1])\n",
    "                # if dict_size < 4096:  # Typically the dictionary size is limited to 4096 entries.\n",
    "                hadict[val][pc] = len(hadict[val])\n",
    "                p = c\n",
    "\n",
    "        if p:\n",
    "            val = ord(c)\n",
    "            result.append(str(hadict[val][p]) + p[-1])\n",
    "        return \"\".join(result)\n",
    "    return lzw_encode(data)\n",
    "def lzw_encode(data):\n",
    "    # Build the dictionary.\n",
    "    global initial_size_lzw \n",
    "    initial_size_lzw = sys.getsizeof(data)\n",
    "    max_dict_size = 256\n",
    "    dictionary = {chr(i): i for i in range(256)}\n",
    "    dict_size = max_dict_size\n",
    "    p = \"\"\n",
    "    result = []\n",
    "\n",
    "    for c in data:\n",
    "        pc = p + c\n",
    "        if pc in dictionary:\n",
    "            p = pc\n",
    "        else:\n",
    "            if len(p) == 1: result.append('#'+p)\n",
    "            else: result.append(str(dictionary[p]))\n",
    "            # if dict_size < 4096:  # Typically the dictionary size is limited to 4096 entries.\n",
    "            dictionary[pc] = str(dict_size)\n",
    "            dict_size += 1\n",
    "            p = c\n",
    "\n",
    "    if p:\n",
    "        result.append(str(dictionary[p]))\n",
    "\n",
    "    return \"\".join(result)\n",
    "def lzw_decode(compressed):\n",
    "    # Build the dictionary.\n",
    "    max_dict_size = 256\n",
    "    dictionary = {i: chr(i) for i in range(max_dict_size)}\n",
    "    dict_size = max_dict_size\n",
    "\n",
    "    # Decoding first value\n",
    "    result = []\n",
    "    w = chr(compressed.pop(0))\n",
    "    result.append(w)\n",
    "\n",
    "    for k in compressed:\n",
    "        if k in dictionary:\n",
    "            entry = dictionary[k]\n",
    "        elif k == dict_size:\n",
    "            entry = w + w[0]\n",
    "        else:\n",
    "            raise ValueError(\"Bad compressed k: %s\" % k)\n",
    "        \n",
    "        result.append(entry)\n",
    "\n",
    "        # Add w+entry[0] to the dictionary.\n",
    "\n",
    "        dictionary[dict_size] = w + entry[0]\n",
    "        dict_size += 1\n",
    "\n",
    "        w = entry\n",
    "\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileId: austen-emma.txt\n",
      "% increase in compression rate:  6.134119397601227\n",
      "% increase in time taken:  32.05158136507759\n",
      "absolute increase in time taken:  0.05858969688415527 seconds\n",
      "\n",
      "FileId: austen-persuasion.txt\n",
      "% increase in compression rate:  6.5136884811757945\n",
      "% increase in time taken:  3.365116978861055\n",
      "absolute increase in time taken:  0.004266023635864258 seconds\n",
      "\n",
      "FileId: austen-sense.txt\n",
      "% increase in compression rate:  5.98361837012737\n",
      "% increase in time taken:  46.89537083807779\n",
      "absolute increase in time taken:  0.06529712677001953 seconds\n",
      "\n",
      "FileId: bible-kjv.txt\n",
      "% increase in compression rate:  5.336814843178569\n",
      "% increase in time taken:  25.08903643181198\n",
      "absolute increase in time taken:  0.25808167457580566 seconds\n",
      "\n",
      "FileId: blake-poems.txt\n",
      "% increase in compression rate:  11.313543793518663\n",
      "% increase in time taken:  32.839080459770116\n",
      "absolute increase in time taken:  0.0027246475219726562 seconds\n",
      "\n",
      "FileId: bryant-stories.txt\n",
      "% increase in compression rate:  8.883393189251588\n",
      "% increase in time taken:  30.251100508225875\n",
      "absolute increase in time taken:  0.014844179153442383 seconds\n",
      "\n",
      "FileId: burgess-busterbrown.txt\n",
      "% increase in compression rate:  10.247662668807253\n",
      "% increase in time taken:  35.04655873395217\n",
      "absolute increase in time taken:  0.005733966827392578 seconds\n",
      "\n",
      "FileId: carroll-alice.txt\n",
      "% increase in compression rate:  10.322339453352164\n",
      "% increase in time taken:  30.648310553661844\n",
      "absolute increase in time taken:  0.008831977844238281 seconds\n",
      "\n",
      "FileId: chesterton-ball.txt\n",
      "% increase in compression rate:  7.047077414687349\n",
      "% increase in time taken:  31.404352936032982\n",
      "absolute increase in time taken:  0.0291140079498291 seconds\n",
      "\n",
      "FileId: chesterton-brown.txt\n",
      "% increase in compression rate:  7.341188852113958\n",
      "% increase in time taken:  31.65545271557991\n",
      "absolute increase in time taken:  0.025384187698364258 seconds\n",
      "\n",
      "FileId: chesterton-thursday.txt\n",
      "% increase in compression rate:  8.036833929139606\n",
      "% increase in time taken:  32.2468319808655\n",
      "absolute increase in time taken:  0.020379304885864258 seconds\n",
      "\n",
      "FileId: edgeworth-parents.txt\n",
      "% increase in compression rate:  7.12558823875356\n",
      "% increase in time taken:  27.03858216831636\n",
      "absolute increase in time taken:  0.05167269706726074 seconds\n",
      "\n",
      "FileId: melville-moby_dick.txt\n",
      "% increase in compression rate:  8.012298890058961\n",
      "% increase in time taken:  29.256524049526373\n",
      "absolute increase in time taken:  0.07692122459411621 seconds\n",
      "\n",
      "FileId: milton-paradise.txt\n",
      "% increase in compression rate:  7.003239590918896\n",
      "% increase in time taken:  27.911035706624542\n",
      "absolute increase in time taken:  0.027298927307128906 seconds\n",
      "\n",
      "FileId: shakespeare-caesar.txt\n",
      "% increase in compression rate:  10.741838178372173\n",
      "% increase in time taken:  34.347545014254074\n",
      "absolute increase in time taken:  0.007727146148681641 seconds\n",
      "\n",
      "FileId: shakespeare-hamlet.txt\n",
      "% increase in compression rate:  10.48425704290186\n",
      "% increase in time taken:  33.745202243873635\n",
      "absolute increase in time taken:  0.010900020599365234 seconds\n",
      "\n",
      "FileId: shakespeare-macbeth.txt\n",
      "% increase in compression rate:  11.606573705179283\n",
      "% increase in time taken:  30.028868360277137\n",
      "absolute increase in time taken:  0.006200075149536133 seconds\n",
      "\n",
      "FileId: whitman-leaves.txt\n",
      "% increase in compression rate:  6.983482926170874\n",
      "% increase in time taken:  26.156379881809045\n",
      "absolute increase in time taken:  0.03926658630371094 seconds\n",
      "\n",
      "\u001b[91mAverage increase in compression rate 8.284308831406063 %\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "# decompressed = lzw_decode(compressed)\n",
    "# print(\"Decompressed:\", decompressed)\n",
    "CompRateInc = []\n",
    "for fileid in fileids:\n",
    "    # if fileid == macbeth:\n",
    "    data = gutenberg.raw(fileid)\n",
    "    print('FileId:', fileid)\n",
    "    start_time_MR = time.time()\n",
    "    encoded_MR = MR(data)\n",
    "    end_time_MR = time.time()\n",
    "    total_time_MR = end_time_MR - start_time_MR\n",
    "    final_size_MR = sys.getsizeof(encoded_MR)\n",
    "    start_time_lzw = time.time()\n",
    "    encoded_lzw = lzw_encode(data)\n",
    "    end_time_lzw = time.time()\n",
    "    total_time_lzw = end_time_lzw - start_time_lzw\n",
    "    final_size_lzw = sys.getsizeof(encoded_lzw)\n",
    "    compMR = (initial_size_MR-final_size_MR)/initial_size_MR * 100\n",
    "    compLZW = (initial_size_lzw-final_size_lzw)/initial_size_lzw * 100\n",
    "    CompInc = compMR - compLZW\n",
    "    CompRateInc.append(CompInc)\n",
    "    # print('Compression Rate MR: ', compMR)\n",
    "    # print('Time Taken MR:', total_time_MR)\n",
    "    # print('Compression Rate LZW: ', compLZW)\n",
    "    # print('Time Taken LZW:', total_time_lzw)\n",
    "    print('% increase in compression rate: ', CompInc)\n",
    "    print('% increase in time taken: ', (total_time_MR-total_time_lzw)/total_time_lzw * 100)\n",
    "    print('absolute increase in time taken: ', total_time_MR - total_time_lzw, 'seconds')\n",
    "    print()\n",
    "print('\\033[91mAverage increase in compression rate', mean(CompRateInc), '%\\033[0m')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
